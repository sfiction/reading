#### 2.3.4 代价敏感错误率与代价曲线

对各种错误设置非均等代价(unequal cost)

？？？？

### 2.4 比较检验

## 3 线性模型

### 3.2 线性回归

最小二乘法

广义线性模型 $y=g^{-1}(\mathbf{\omega}^{T}\mathcal{x}+b)$

### 3.3 对数几率回归

单位跃阶函数、logstic 函数与 Sigmoid 函数

## 4 决策树

决定划分属性和样本划分。图 4.2 决策树学习基本算法。注意 line 12 应当没有 return。

信息熵与信息增益，ID3 决策树算法。

增益率，$IV(a)$。C4.5 算法，选择信息增益高于平均的属性，再选择最高增益率。

基尼指数与 CART 决策树。

预剪枝和后剪枝方法都比较分支生成前后的模型准确率。前者在展开前进行，后者在生成整棵决策树后进行。

连续值处理：比较连续值的划分点集合。在后代结点中仍可作为划分属性。

缺失值处理：为每个样本赋予权重 $w_x$，计算信息增益时仅考虑无缺失样本集合，再乘上其所占比例。样本划分时，缺失样本放入所有分支，权值乘上该分支无缺失样本所占比例。

多变量决策树：每个结点对应一个所有属性的线性分类器，而不是某属性的最优划分。

## 5 神经网络



## 6 支持向量机



## 16 强化学习

### 16.1 任务与奖赏

强化学习中机器处于环境 $E=\langle X, A, P, R \rangle$，能感知到状态 $x \in X$，并可选择动作 $a \in A$。潜在的转移概率函数为 $P:X \times A \times X \mapsto \mathbb{R}$，潜在的奖赏函数  $R:X \times A \times X \mapsto \mathbb{R}$。

机器需要学习得到一个策略（policy）$a=\pi(x)$ 来决定在某个状态下的动作。策略也可能是非确定性的，即用 $\pi(x,a)$ 表示状态 $x$ 下选择动作 $a$ 的概率。

学习目标是获得使长期积累奖赏最大化的策略。

长期积累奖赏有多重计算方式，常用的如 $T$ 步累积奖赏和 $\gamma$ 折扣累积奖赏（指数衰减）。

### 16.2 K-摇臂赌博机

考虑一个简单情形，只最大化单步奖赏。每个操作的回报有一定概率分布，需要在测试操作的期望回报和进行当前最优操作之间权衡。

探索

- $\epsilon$-贪心, 以 $\epsilon$ 的概率进行探索（均匀随机选取操作），否则选择当前平均奖赏最高的操作。
  - 可以令 $\epsilon$ 相对操作次数变化，不一定要取常数，例如 $\epsilon=1/\sqrt{t}$
- Softmax, $w(x)=e^{\frac{Q(k)}{\tau}}, P(k)=\frac{w(x)}{\sum_{i=1}^{K}{w(x)}}$
  - $\tau$ 可以调整算法在仅探索和仅利用之间的平衡
  - Boltzmann 分布

### 16.3 有模型学习

假设多步强化学习任务对应的马尔科夫决策过程（Markov Decision Process）四元组已知。用状态值函数 $V^{\pi}(x)$ 表示策略 $\pi$ 在状态 $x$ 上的奖赏，状态-动作值函数 $Q^{\pi}(x,a)$ 在 $x$ 上执行动作 $a$ 再使用策略 $\pi$ 的奖赏。定义相应的 $T$ 步累计奖赏和 $\gamma$ 折扣累积奖赏，容易写出其递归式。

用递推式可以迭代多轮求出状态值函数。

显然最优策略下对应的值函数只选择期望收益最大的下一步。可以用更改到当前状态最优动作的方式改进策略。

不涉及泛化能力，只是为每个状态找到最优动作。

### 16.4 免模型学习

#### 16.4.1 蒙特卡罗强化学习

从起始状态出发，按照某种策略执行 $T$ 步获得轨迹。用采样轨迹的累积奖赏来更新状态-动作值函数。

采样过程面临和 K 摇臂赌博机类似的问题，需要平衡探索和利用。以采用 $\epsilon$-贪心法为例，以 $\epsilon$ 的概率均匀随机选择，否则按照确定性策略 $\pi$ 选择。每采样一条轨迹，就更新一遍状态-动作值函数，并用值函数更新策略。

因为采样时使用的策略和每步改进的策略函数是同一个，都是 $\epsilon$-贪心策略，这种方法被称为同策略蒙特卡罗强化学习。

如果要改进原始策略，可以用 $\epsilon$-贪心策略产生轨迹的概率和原始策略产生轨迹的概率修正累积奖赏，用前者上的采样结果来估计后者的值函数。

QNS: 为什么原始策略的轨迹概率可以全部取 1？？没有 0？？

#### 16.4.2 时序差分学习

每进行一次决策就更新值函数和策略，而不是在获得整条轨迹后再更新。

感觉蒙特卡罗强化学习更适合 $T$ 步累积奖赏，时序差分学习更适合 $\gamma$ 折扣累积奖赏。

### 16.5 值函数近似

简单情形：值函数是状态的线性函数 $V_{\boldsymbol\theta}(\boldsymbol{x})=\boldsymbol\theta^{\mathrm{T}}\boldsymbol{x}$



### 16.6 模仿学习









