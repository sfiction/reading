

### Ch2 反向传播算法如何工作

链式法则？

### Ch3 改进神经网络的学习方法

#### 3.1 交叉熵代价函数

$cost=\sum{y_i\log{a_i}}$

MSE 对输出层的权重的偏导中有 $\sigma(z)$，在 z 偏离很大时，S 型函数的导数值趋于 0。换用交叉熵一通求导，。QNS: 可是这只解决了最后一层的问题？

#### 3.5 如何选择神经网络的超参数

- 宽泛策略, 从更简化的问题入手/提高训练过程中准确率反馈结果的密度
- 提前终止, n 回合不提升即终止
- 学习速率, 可变学习速率，一种方案是在验证准确率变差时降低学习速率
- 规范化参数, 先用不含规范化的优化函数获得一个系数的规范化值，以它为基准调整得到合适的规范化系数
- 批量数据大小, 较大能充分利用并行性能，较小时能有更高的更新速率
- 自动技术, 如网格搜索（Grid Search）

#### 3.6 其他技术

##### 3.6.1 随机梯度下降的变化形式

- Hessian, 优化函数泰勒展开的 Hessian 矩阵
- 基于 Momentum 的梯度下降, 引入速度概念

##### 3.6.2 人工神经元的其他模型

- tanh, $\tanh(z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$
- ReLU, $\max(0,w \cdot x + b)$

### Ch4 神经网络可以计算任何函数的可视化证明

对单输入和单输出证明，作者只是把每个神经元调整为极陡的阶跃函数，用一对神经元表示一个平顶的峰，然后用来分段近似。。

整体是个比较粗糙的构造，还是看看原证明比较好。。

### Ch5 深度神经网络为何很难训练

#### 5.2 什么导致了梯度消失

损失函数对偏置的导数是路径上激活函数导数和边权的乘积。Sigmoid 函数的导数小于 1/4，如果假设边权变化不大，那么单条路径贡献的梯度会逐渐变小。（但是路径数也变多了？？）

梯度激增：边权很大时

$\left| w \cdot \sigma'(wa+b)\right| \le 1$ 的范围相当小。

习题暗示了 ResNet？

### Ch6 深度学习

#### 6.1 卷积神经网络

问题-卷积网络中的反向传播：卷积输出层的误差值与输入层的卷积